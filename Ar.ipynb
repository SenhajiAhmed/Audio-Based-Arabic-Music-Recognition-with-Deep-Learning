{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334e148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 21:23:39.390022: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-29 21:23:39.396532: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-29 21:23:40.034578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-29 21:23:43.084514: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-29 21:23:43.085714: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-29 21:23:45.918680: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/senhajiahmed/.local/lib/python3.13/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 18 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load trained model\n",
    "# -------------------------------\n",
    "model_path = \"arabic_music_genre_model.keras\"  # path to your model\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a49c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050  # sampling rate\n",
    "DURATION = 30  # Changed from 10 to 30 seconds per clip\n",
    "SAMPLES_PER_TRACK = SR * DURATION\n",
    "N_MFCC = 13  # number of MFCCs\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 5169  # Modified for 30-second clips: (22050*30)/5169 ≈ 128 → padded to 130\n",
    "\n",
    "def extract_features(audio_signal):\n",
    "    \"\"\"Process audio signal instead of file path\"\"\"\n",
    "    try:\n",
    "        # Ensure correct length\n",
    "        signal = librosa.util.fix_length(audio_signal, size=SAMPLES_PER_TRACK)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=signal, sr=SR, n_mfcc=N_MFCC,\n",
    "                                   n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "\n",
    "        # Pad/trim to exact 130 time steps\n",
    "        if mfcc.shape[1] < 130:\n",
    "            mfcc = np.pad(mfcc, ((0,0), (0,130 - mfcc.shape[1])))\n",
    "        else:\n",
    "            mfcc = mfcc[:, :130]\n",
    "\n",
    "        # Transpose and add channel dimension\n",
    "        mfcc = mfcc.T[..., np.newaxis]  # (130, 13, 1)\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d0c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_names = ['east', 'loyal', 'Muwa', 'poems', 'rai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_any_audio(file_path, chunk_size=30):  # Changed default chunk_size to 30\n",
    "    try:\n",
    "        # Load full audio\n",
    "        print(f\"Loading audio file: {file_path}\")\n",
    "        full_signal, sr = librosa.load(file_path, sr=SR)\n",
    "        total_samples = len(full_signal)\n",
    "        total_duration = total_samples / SR\n",
    "\n",
    "        print(f\"Audio duration: {total_duration:.2f} seconds\")\n",
    "        print(f\"Processing in {chunk_size}-second chunks\")\n",
    "\n",
    "        predictions = []\n",
    "        chunk_results = []  # To store individual chunk predictions\n",
    "\n",
    "        # Process audio in 30-second chunks\n",
    "        for start_sample in range(0, total_samples, SR*chunk_size):\n",
    "            end_sample = min(start_sample + SR*chunk_size, total_samples)\n",
    "            chunk_duration = (end_sample - start_sample) / SR\n",
    "\n",
    "            # Skip chunks that are too short (less than 5 seconds)\n",
    "            if chunk_duration < 5:\n",
    "                print(f\"Skipping final chunk ({chunk_duration:.2f}s) - too short\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing chunk {len(predictions)+1}: {start_sample/SR:.2f}s - {end_sample/SR:.2f}s\")\n",
    "\n",
    "            chunk = full_signal[start_sample:end_sample]\n",
    "\n",
    "            # Process chunk\n",
    "            features = extract_features(chunk)\n",
    "            if features is None:\n",
    "                print(f\"Skipping invalid chunk {start_sample/SR:.2f}s - {end_sample/SR:.2f}s\")\n",
    "                continue\n",
    "\n",
    "            # Predict\n",
    "            pred = model.predict(features[np.newaxis, ...], verbose=0)\n",
    "            predicted_index = np.argmax(pred)\n",
    "            predicted_genre = genre_names[predicted_index]\n",
    "            confidence = float(pred[0][predicted_index])\n",
    "\n",
    "            print(f\"  Chunk result: {predicted_genre} (confidence: {confidence:.2f})\")\n",
    "\n",
    "            predictions.append(predicted_index)\n",
    "            chunk_results.append({\n",
    "                \"start_time\": start_sample/SR,\n",
    "                \"end_time\": end_sample/SR,\n",
    "                \"genre\": predicted_genre,\n",
    "                \"confidence\": confidence,\n",
    "                \"all_confidences\": {genre_names[i]: float(pred[0][i]) for i in range(len(genre_names))}\n",
    "            })\n",
    "\n",
    "        if not predictions:\n",
    "            return \"No valid predictions\"\n",
    "\n",
    "        # Get most common genre\n",
    "        most_common_genre_index = np.bincount(predictions).argmax()\n",
    "        most_common_genre = genre_names[most_common_genre_index]\n",
    "\n",
    "        # Calculate average confidence for each genre\n",
    "        genre_confidences = {}\n",
    "        for chunk in chunk_results:\n",
    "            for genre, conf in chunk[\"all_confidences\"].items():\n",
    "                if genre not in genre_confidences:\n",
    "                    genre_confidences[genre] = []\n",
    "                genre_confidences[genre].append(conf)\n",
    "\n",
    "        avg_confidences = {genre: np.mean(confs) for genre, confs in genre_confidences.items()}\n",
    "        sorted_confidences = sorted(avg_confidences.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"predicted_genre\": most_common_genre,\n",
    "            \"prediction_count\": {genre_names[i]: np.bincount(predictions)[i] if i < len(np.bincount(predictions)) else 0\n",
    "                               for i in range(len(genre_names))},\n",
    "            \"average_confidences\": sorted_confidences,\n",
    "            \"chunk_results\": chunk_results\n",
    "        }\n",
    "\n",
    "        print(\"\\n===== RESULTS =====\")\n",
    "        print(f\"Predicted genre: {result['predicted_genre']}\")\n",
    "        print(f\"Processed {len(chunk_results)} chunks of {chunk_size} seconds each\")\n",
    "\n",
    "        print(\"\\nGenre distribution across chunks:\")\n",
    "        for genre, count in result['prediction_count'].items():\n",
    "            if count > 0:\n",
    "                print(f\"  {genre}: {count} chunks ({count/len(chunk_results)*100:.1f}%)\")\n",
    "\n",
    "        print(\"\\nAverage confidence scores:\")\n",
    "        for genre, conf in sorted_confidences:\n",
    "            print(f\"  {genre}: {conf*100:.2f}%\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Processing error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39840eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio file: niama.wav\n",
      "Audio duration: 1174.34 seconds\n",
      "Processing in 30-second chunks\n",
      "Processing chunk 1: 0.00s - 30.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 21:23:51.242440: I external/local_xla/xla/service/service.cc:163] XLA service 0x7fea58011fa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-29 21:23:51.242470: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): Host, Default Version\n",
      "2025-08-29 21:23:51.261303: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756499031.485434    6542 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk result: loyal (confidence: 0.95)\n",
      "Processing chunk 2: 30.00s - 60.00s\n",
      "  Chunk result: loyal (confidence: 1.00)\n",
      "Processing chunk 3: 60.00s - 90.00s\n",
      "  Chunk result: loyal (confidence: 0.93)\n",
      "Processing chunk 4: 90.00s - 120.00s\n",
      "  Chunk result: loyal (confidence: 0.99)\n",
      "Processing chunk 5: 120.00s - 150.00s\n",
      "  Chunk result: rai (confidence: 0.58)\n",
      "Processing chunk 6: 150.00s - 180.00s\n",
      "  Chunk result: loyal (confidence: 1.00)\n",
      "Processing chunk 7: 180.00s - 210.00s\n",
      "  Chunk result: poems (confidence: 0.81)\n",
      "Processing chunk 8: 210.00s - 240.00s\n",
      "  Chunk result: poems (confidence: 0.95)\n",
      "Processing chunk 9: 240.00s - 270.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 10: 270.00s - 300.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 11: 300.00s - 330.00s\n",
      "  Chunk result: poems (confidence: 1.00)\n",
      "Processing chunk 12: 330.00s - 360.00s\n",
      "  Chunk result: loyal (confidence: 0.98)\n",
      "Processing chunk 13: 360.00s - 390.00s\n",
      "  Chunk result: rai (confidence: 0.76)\n",
      "Processing chunk 14: 390.00s - 420.00s\n",
      "  Chunk result: poems (confidence: 1.00)\n",
      "Processing chunk 15: 420.00s - 450.00s\n",
      "  Chunk result: rai (confidence: 0.95)\n",
      "Processing chunk 16: 450.00s - 480.00s\n",
      "  Chunk result: poems (confidence: 0.99)\n",
      "Processing chunk 17: 480.00s - 510.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 18: 510.00s - 540.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 19: 540.00s - 570.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 20: 570.00s - 600.00s\n",
      "  Chunk result: loyal (confidence: 1.00)\n",
      "Processing chunk 21: 600.00s - 630.00s\n",
      "  Chunk result: rai (confidence: 0.94)\n",
      "Processing chunk 22: 630.00s - 660.00s\n",
      "  Chunk result: poems (confidence: 0.83)\n",
      "Processing chunk 23: 660.00s - 690.00s\n",
      "  Chunk result: rai (confidence: 0.99)\n",
      "Processing chunk 24: 690.00s - 720.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 25: 720.00s - 750.00s\n",
      "  Chunk result: poems (confidence: 1.00)\n",
      "Processing chunk 26: 750.00s - 780.00s\n",
      "  Chunk result: loyal (confidence: 0.98)\n",
      "Processing chunk 27: 780.00s - 810.00s\n",
      "  Chunk result: loyal (confidence: 1.00)\n",
      "Processing chunk 28: 810.00s - 840.00s\n",
      "  Chunk result: loyal (confidence: 0.98)\n",
      "Processing chunk 29: 840.00s - 870.00s\n",
      "  Chunk result: poems (confidence: 0.98)\n",
      "Processing chunk 30: 870.00s - 900.00s\n",
      "  Chunk result: loyal (confidence: 0.93)\n",
      "Processing chunk 31: 900.00s - 930.00s\n",
      "  Chunk result: rai (confidence: 0.96)\n",
      "Processing chunk 32: 930.00s - 960.00s\n",
      "  Chunk result: loyal (confidence: 0.79)\n",
      "Processing chunk 33: 960.00s - 990.00s\n",
      "  Chunk result: loyal (confidence: 0.95)\n",
      "Processing chunk 34: 990.00s - 1020.00s\n",
      "  Chunk result: rai (confidence: 0.96)\n",
      "Processing chunk 35: 1020.00s - 1050.00s\n",
      "  Chunk result: loyal (confidence: 0.53)\n",
      "Processing chunk 36: 1050.00s - 1080.00s\n",
      "  Chunk result: poems (confidence: 0.96)\n",
      "Processing chunk 37: 1080.00s - 1110.00s\n",
      "  Chunk result: loyal (confidence: 0.96)\n",
      "Processing chunk 38: 1110.00s - 1140.00s\n",
      "  Chunk result: loyal (confidence: 1.00)\n",
      "Processing chunk 39: 1140.00s - 1170.00s\n",
      "  Chunk result: poems (confidence: 0.96)\n",
      "Skipping final chunk (4.34s) - too short\n",
      "\n",
      "===== RESULTS =====\n",
      "Predicted genre: loyal\n",
      "Processed 39 chunks of 30 seconds each\n",
      "\n",
      "Genre distribution across chunks:\n",
      "  loyal: 16 chunks (41.0%)\n",
      "  poems: 10 chunks (25.6%)\n",
      "  rai: 13 chunks (33.3%)\n",
      "\n",
      "Average confidence scores:\n",
      "  loyal: 40.70%\n",
      "  rai: 34.49%\n",
      "  poems: 24.81%\n",
      "  Muwa: 0.00%\n",
      "  east: 0.00%\n",
      "\n",
      "Predicted genre: loyal\n",
      "Average confidences:\n",
      "  loyal: 40.70%\n",
      "  rai: 34.49%\n",
      "  poems: 24.81%\n",
      "  Muwa: 0.00%\n",
      "  east: 0.00%\n"
     ]
    }
   ],
   "source": [
    "file_path = \"niama.wav\"  # replace with your file\n",
    "result = process_any_audio(file_path)\n",
    "if isinstance(result, dict):\n",
    "    print(f\"\\nPredicted genre: {result['predicted_genre']}\")\n",
    "    print(\"Average confidences:\")\n",
    "    for genre, conf in result[\"average_confidences\"]:\n",
    "        print(f\"  {genre}: {conf*100:.2f}%\")\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40bb9880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio file: khaled.wav\n",
      "Audio duration: 327.56 seconds\n",
      "Processing in 30-second chunks\n",
      "Processing chunk 1: 0.00s - 30.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 2: 30.00s - 60.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 3: 60.00s - 90.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 4: 90.00s - 120.00s\n",
      "  Chunk result: rai (confidence: 0.88)\n",
      "Processing chunk 5: 120.00s - 150.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 6: 150.00s - 180.00s\n",
      "  Chunk result: rai (confidence: 0.69)\n",
      "Processing chunk 7: 180.00s - 210.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 8: 210.00s - 240.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 9: 240.00s - 270.00s\n",
      "  Chunk result: rai (confidence: 1.00)\n",
      "Processing chunk 10: 270.00s - 300.00s\n",
      "  Chunk result: rai (confidence: 0.91)\n",
      "Processing chunk 11: 300.00s - 327.56s\n",
      "  Chunk result: rai (confidence: 0.99)\n",
      "\n",
      "===== RESULTS =====\n",
      "Predicted genre: rai\n",
      "Processed 11 chunks of 30 seconds each\n",
      "\n",
      "Genre distribution across chunks:\n",
      "  rai: 11 chunks (100.0%)\n",
      "\n",
      "Average confidence scores:\n",
      "  rai: 95.17%\n",
      "  poems: 2.99%\n",
      "  loyal: 1.84%\n",
      "  east: 0.00%\n",
      "  Muwa: 0.00%\n",
      "\n",
      "Predicted genre: rai\n",
      "Average confidences:\n",
      "  rai: 95.17%\n",
      "  poems: 2.99%\n",
      "  loyal: 1.84%\n",
      "  east: 0.00%\n",
      "  Muwa: 0.00%\n"
     ]
    }
   ],
   "source": [
    "file_path = \"khaled.wav\"  # replace with your file\n",
    "result = process_any_audio(file_path)\n",
    "if isinstance(result, dict):\n",
    "    print(f\"\\nPredicted genre: {result['predicted_genre']}\")\n",
    "    print(\"Average confidences:\")\n",
    "    for genre, conf in result[\"average_confidences\"]:\n",
    "        print(f\"  {genre}: {conf*100:.2f}%\")\n",
    "else:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
